<?xml version="1.0"?>
<job_conf>
    <plugins workers="4">
        <!-- "workers" is the number of threads for the runner's work queue.
             The default from <plugins> is used if not defined for a <plugin>.
          -->
        <plugin id="pbs" type="runner" load="galaxy.jobs.runners.pbs:PBSJobRunner" workers="8"/>
        <plugin id="cli" type="runner" load="galaxy.jobs.runners.cli:ShellJobRunner" />
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="dynamic" type="runner">
            <!-- The dynamic runner is not a real job running plugin and is
                 always loaded, so it does not need to be explicitly stated in
                 <plugins>. However, if you wish to change the base module
                 containing your dynamic rules, you can do so.

                 The `load` attribute is not required (and ignored if
                 included).
            -->
            <param id="rules_module">galaxy.jobs.rules</param>
        </plugin>
    </plugins>
    <handlers default="handlers">
        <!-- Additional job handlers - the id should match the name of a
             [server:<id>] in galaxy.ini.
         -->
        <handler id="handler0" tags="handlers"/>
        <handler id="handler1" tags="handlers"/>
        <handler id="handler2" tags="handlers"/>
        <handler id="handler3" tags="handlers"/>
        <handler id="handler4" tags="handlers"/>
        <handler id="handler5" tags="handlers"/>
        <handler id="handler6" tags="handlers"/>
        <handler id="handler7" tags="handlers"/>
        <!-- Handlers will load all plugins defined in the <plugins> collection
             above by default, but can be limited to a subset using <plugin>
             tags. This is useful for heterogenous environments where the DRMAA
             plugin would need to be loaded more than once with different
             configs.
         -->
        <!--
        <handler id="special_handler0" tags="special_handlers"/>
        <handler id="special_handler1" tags="special_handlers"/>
        <handler id="trackster_handler"/>
        -->
    </handlers>
    <destinations default="pbs_avgmem">
        <!-- Destinations define details about remote resources and how jobs
             should be executed on those remote resources.
         -->
        <destination id="local" runner="local">
            <env file="/gpfs/cyberstar/pughhpc/galaxy-cegr/galaxy/.venv/bin/activate" />
            <param id="Resource_List">walltime=1:00:00,ncpus=4</param>
            <env id="_JAVA_OPTIONS">-Djava.io.tmpdir=/gpfs/cyberstar/pughhpc/galaxy-cegr/files/tmp -Xmx7680m -Xms256m</env>
        </destination>
        <destination id="multicore_local" runner="local">
          <param id="local_slots">4</param> <!-- Specify GALAXY_SLOTS for local jobs. -->
          <!-- Warning: Local slot count doesn't tie up additional worker threads, to prevent over
               allocating machine define a second local runner with different name and fewer workers
               to run this destination. -->
          <param id="embed_metadata_in_job">True</param>
          <!-- Above parameter will be default (with no option to set
               to False) in an upcoming release of Galaxy, but you can
               try it early - it will slightly speed up local jobs by
               embedding metadata calculation in job script itself.
          -->
          <env file="/gpfs/cyberstar/pughhpc/galaxy-cegr/galaxy/.venv/bin/activate" />
          <job_metrics />
          <!-- Above element demonstrates embedded job metrics definition - see
               job_metrics_conf.xml.sample for full documentation on possible nested
               elements. This empty block will simply disable job metrics for the
               corresponding destination. -->
        </destination>
        <destination id="pbs_avgmem" runner="pbs" tags="avgmem">
            <env file="/gpfs/cyberstar/pughhpc/galaxy-cegr/galaxy/.venv/bin/activate" />
            <env id="TEMP">/gpfs/cyberstar/pughhpc/galaxy-cegr/files/tmp</env>
            <env id="TMPDIR">/gpfs/cyberstar/pughhpc/galaxy-cegr/files/tmp</env>
            <param id="Resource_List">walltime=48:00:00,ncpus=4,mem=8gb</param>
            <param id="-q">lionxg-pughhpc@lionxg.rcc.psu.edu</param>
        </destination>
        <destination id="pbs_highmem" runner="pbs" tags="highmem">
            <env file="/gpfs/cyberstar/pughhpc/galaxy-cegr/galaxy/.venv/bin/activate" />
            <env id="TEMP">/gpfs/cyberstar/pughhpc/galaxy-cegr/files/tmp</env>
            <env id="TMPDIR">/gpfs/cyberstar/pughhpc/galaxy-cegr/files/tmp</env>
            <env id="_JAVA_OPTIONS">-Djava.io.tmpdir=/gpfs/cyberstar/pughhpc/galaxy-cegr/files/tmp -Xmx15872m -Xms256m</env>
            <param id="Resource_List">walltime=48:00:00,ncpus=4,mem=16gb</param>
            <param id="-q">lionxg-pughhpc@lionxg.rcc.psu.edu</param>
        </destination>
        <destination id="dynamic" runner="dynamic">
            <!-- A destination that represents a method in the dynamic runner. -->
            <param id="function">foo</param>
        </destination>
        <destination id="load_balance" runner="dynamic">
            <param id="type">choose_one</param>
            <!-- Randomly assign jobs to various static destination ids -->
            <param id="destination_ids">cluster1,cluster2,cluster3</param>
        </destination>
        <destination id="load_balance_with_data_locality" runner="dynamic">
            <!-- Randomly assign jobs to various static destination ids,
                 but keep jobs in the same workflow invocation together and
                 for those jobs ran outside of workflows keep jobs in same
                 history together.
            -->
            <param id="type">choose_one</param>
            <param id="destination_ids">cluster1,cluster2,cluster3</param>
            <param id="hash_by">workflow_invocation,history</param>
        </destination>
        <destination id="burst_out" runner="dynamic">
            <!-- Burst out from static destination local_cluster_8_core to
            static destination shared_cluster_8_core when there are about
            50 Galaxy jobs assigned to any of the local_cluster_XXX
            destinations (either running or queued). If there are fewer
            than 50 jobs, just use local_cluster_8_core destination.

            Uncomment job_state parameter to make this bursting happen when
            roughly 50 jobs are queued instead.
            -->
            <param id="type">burst</param>
            <param id="from_destination_ids">local_cluster_8_core,local_cluster_1_core,local_cluster_16_core</param>
            <param id="to_destination_id">shared_cluster_8_core</param>
            <param id="num_jobs">50</param>
            <!-- <param id="job_states">queued</param> -->
        </destination>
        <destination id="ssh_torque" runner="cli">
            <param id="shell_plugin">SecureShell</param>
            <param id="job_plugin">Torque</param>
            <param id="shell_username">foo</param>
            <param id="shell_hostname">foo.example.org</param>
            <param id="job_Resource_List">walltime=48:00:00,ncpus=4</param>
        </destination>

        <!-- Jobs that hit the walltime on one destination can be automatically
             resubmitted to another destination. Walltime detection is
             currently only implemented in the slurm runner.

             Multiple resubmit tags can be defined, the first resubmit matching
             the terminal condition of a job will be used.

             The 'condition' attribute is optional, if not present, the
             resubmit destination will be used for all conditions. The
             conditions currently implemented are:

               - "walltime_reached"
               - "memory_limit_reached"

             The 'handler' tag is optional, if not present, the job's original
             handler will be reused for the resubmitted job.
        -->
        <!-- Any tag param in this file can be set using an environment variable or using
             values from galaxy.ini using the from_environ and from_config attributes
             repectively. The text of the param will still be used if that environment variable
             or config value isn't set.
        -->
        <!-- Templatized destinations - macros can be used to create templated
        destinations with reduced XML duplication. Here we are creating 4 destinations in 4 lines instead of 28 using the macros defined below.
        -->
        <expand macro="foohost_destination" id="foo_small" ncpus="1" walltime="1:00:00" />
        <expand macro="foohost_destination" id="foo_medium" ncpus="2" walltime="4:00:00" />
        <expand macro="foohost_destination" id="foo_large" ncpus="8" walltime="24:00:00" />
        <expand macro="foohost_destination" id="foo_longrunning" ncpus="1" walltime="48:00:00" />
    </destinations>
    <resources default="default">
      <!-- Group different parameters defined in job_resource_params_conf.xml
           together and assign these groups ids. Tool section below can map
           tools to different groups. This is experimental functionality!
      -->
      <group id="default"></group>
      <group id="memoryonly">memory</group>
      <group id="all">processors,memory,time,project</group>
    </resources>
    <tools>
        <!-- Tools can be configured to use specific destinations or handlers,
             identified by either the "id" or "tags" attribute.  If assigned to
             a tag, a handler or destination that matches that tag will be
             chosen at random.
         -->
        <!-- <tool id="upload1" handler="handler0" destination="local" /> -->
        <!-- <tool id="bar" destination="dynamic"/> -->
        <!-- Next example defines resource group to insert into tool interface
             and pass to dynamic destination (as resource_params argument). -->
        <!-- <tool id="longbar" destination="dynamic" resources="all" /> -->
        <!-- <tool id="baz" handler="special_handlers" destination="bigmem"/> -->
        <tool id="toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.12.1" destination="pbs_highmem" resources="all"/>
        <tool id="toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/1.136.0" destination="pbs_highmem" resources="all"/>
        <tool id="toolshed.g2.bx.psu.edu/repos/bgruening/repeat_masker/repeatmasker_wrapper/0.1.2" destination="pbs_highmem" resources="all"/>
    </tools>
    <limits>
        <!-- Certain limits can be defined. The 'concurrent_jobs' limits all
             control the number of jobs that can be "active" at a time, that
             is, dispatched to a runner and in the 'queued' or 'running'
             states.

             A race condition exists that will allow destination_* concurrency
             limits to be surpassed when multiple handlers are allowed to
             handle jobs for the same destination. To prevent this, assign all
             jobs for a specific destination to a single handler.
        -->
        <!-- registered_user_concurrent_jobs:
                Limit on the number of jobs a user with a registered Galaxy
                account can have active across all destinations.
        -->
        <limit type="registered_user_concurrent_jobs">50</limit>
        <!-- anonymous_user_concurrent_jobs:
                Likewise, but for unregistered/anonymous users.
        -->
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <!-- destination_user_concurrent_jobs:
                The number of jobs a user can have active in the specified
                destination, or across all destinations identified by the
                specified tag. (formerly: concurrent_jobs)
        -->
        <limit type="destination_user_concurrent_jobs" tag="avgmem">50</limit>
        <limit type="destination_user_concurrent_jobs" tag="highmem">20</limit>
        <!-- destination_total_concurrent_jobs:
                The number of jobs that can be active in the specified
                destination (or across all destinations identified by the
                specified tag) by any/all users.
        -->
        <limit type="destination_total_concurrent_jobs" tag="highmem">20</limit>
        <!-- walltime:
                Amount of time a job can run (in any destination) before it
                will be terminated by Galaxy.
         -->
        <limit type="walltime">48:00:00</limit>
        <!-- output_size:
                Size that any defined tool output can grow to before the job
                will be terminated. This does not include temporary files
                created by the job. Format is flexible, e.g.:
                '10GB' = '10g' = '10240 Mb' = '10737418240'
        -->
        <limit type="output_size">500GB</limit>
    </limits>
    <macros>
        <xml name="foohost_destination" tokens="id,walltime,ncpus">
            <destination id="@ID@" runner="cli">
                <param id="shell_plugin">SecureShell</param>
                <param id="job_plugin">Torque</param>
                <param id="shell_username">galaxy</param>
                <param id="shell_hostname">foohost_destination.example.org</param>
                <param id="job_Resource_List">walltime=@WALLTIME@,ncpus=@NCPUS@</param>
            </destination>
        </xml>
    </macros>
</job_conf>
